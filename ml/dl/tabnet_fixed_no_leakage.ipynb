{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet 폐업률 예측 모델 - Data Leakage 완전 제거 버전\n",
    "\n",
    "**주요 수정사항:**\n",
    "1. 타겟 변수: t 시점 데이터 → t+1 시점 폐업 위험 예측\n",
    "2. 폐업_률, 폐업_점포_수 특성 완전 제거\n",
    "3. 현재 분기(t) 데이터 사용 금지 - 오직 과거(t-1, t-2) 데이터만 사용\n",
    "4. 그룹 통계량은 train에서만 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 라이브러리 임포트 및 GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mac GPU 사용\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시드 설정\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# GPU 설정\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"✓ Mac GPU 사용\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"✓ NVIDIA GPU 사용\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"✓ CPU 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. 데이터 로드\n",
      "================================================================================\n",
      "✓ 데이터 Shape: (39975, 137)\n",
      "✓ 기간: 20191 ~ 20252\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. 데이터 로드\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# merged_data.csv 있는 폴더 경로 -> 수정 필요\n",
    "data_path = '/Users/kimjm/Desktop/SKN_19/SKN19_2nd_1team/eda/data/merged_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"✓ 데이터 Shape: {df.shape}\")\n",
    "print(f\"✓ 기간: {df['기준_년분기_코드'].min()} ~ {df['기준_년분기_코드'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 타겟 변수 생성 (미래 시점 예측)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. 타겟 변수 생성\n",
      "================================================================================\n",
      "✓ 폐업률 임계값: 2.30%\n",
      "✓ 클래스 분포:\n",
      "closure_risk\n",
      "1    20320\n",
      "0    19655\n",
      "Name: count, dtype: int64\n",
      "✓ 타겟 생성 후 데이터 Shape: (39975, 144)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. 타겟 변수 생성\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 시간 변수 생성\n",
    "df['year'] = df['기준_년분기_코드'].astype(str).str[:4].astype(int)\n",
    "df['quarter'] = df['기준_년분기_코드'].astype(str).str[4:].astype(int)\n",
    "df['year_quarter'] = df['year'] * 10 + df['quarter']\n",
    "\n",
    "# 범주형 인코딩\n",
    "le_district = LabelEncoder()\n",
    "df['자치구_encoded'] = le_district.fit_transform(df['자치구_코드_명'])\n",
    "\n",
    "le_industry = LabelEncoder()\n",
    "df['업종_encoded'] = le_industry.fit_transform(df['서비스_업종_코드_명'])\n",
    "\n",
    "le_change = LabelEncoder()\n",
    "df['상권변화_encoded'] = le_change.fit_transform(df['상권_변화_지표'])\n",
    "\n",
    "# 시계열 정렬\n",
    "df = df.sort_values(['자치구_encoded', '업종_encoded', 'year_quarter']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# 타겟 변수 생성 (전체 데이터의 중앙값 기준)\n",
    "threshold = df['폐업_률'].quantile(0.5)\n",
    "df['closure_risk'] = (df['폐업_률'] >= threshold).astype(int)\n",
    "\n",
    "print(f\"✓ 폐업률 임계값: {threshold:.2f}%\")\n",
    "print(f\"✓ 클래스 분포:\")\n",
    "print(df['closure_risk'].value_counts())\n",
    "\n",
    "# 타겟이 없는 마지막 분기 데이터 제거\n",
    "df = df.dropna(subset=['폐업_률']).reset_index(drop=True)\n",
    "print(f\"✓ 타겟 생성 후 데이터 Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 특성 생성 - 과거 시점 데이터만 사용 (Data Leakage 방지)\n",
    "\n",
    "**중요:**\n",
    "- 🚫 폐업_률, 폐업_점포_수는 절대 사용하지 않음\n",
    "- ✅ 오직 t-1, t-2 시점 데이터만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. 특성 생성\n",
      "================================================================================\n",
      "✓ Lag 특성 생성 대상: 20개 컬럼\n",
      "✓ Lag 특성 생성 완료: 40개 특성 추가\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. 특성 생성\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 🚫 폐업_률, 폐업_점포_수는 절대 사용하지 않음!\n",
    "lag_cols = [\n",
    "    '당월_매출_금액', '당월_매출_건수', '점포_수', '유사_업종_점포_수',\n",
    "    '프랜차이즈_점포_수', '개업_률', '개업_점포_수',\n",
    "    '전체임대료', '총_유동인구_수', '총_상주인구_수', '총_직장인구_수',\n",
    "    '토요일_매출_금액', '일요일_매출_금액', '시간대_21_24_매출_금액',\n",
    "    '연령대_10_매출_금액', '연령대_20_매출_금액', '연령대_30_매출_금액',\n",
    "    '연령대_40_매출_금액', '연령대_50_매출_금액', '연령대_60_이상_매출_금액'\n",
    "]\n",
    "\n",
    "print(f\"✓ Lag 특성 생성 대상: {len(lag_cols)}개 컬럼\")\n",
    "\n",
    "# Lag 특성 생성 (t-1, t-2)\n",
    "for col in lag_cols:\n",
    "    df[f'{col}_lag1'] = df.groupby(['자치구_encoded', '업종_encoded'])[col].shift(1)\n",
    "    df[f'{col}_lag2'] = df.groupby(['자치구_encoded', '업종_encoded'])[col].shift(2)\n",
    "\n",
    "print(f\"✓ Lag 특성 생성 완료: {len(lag_cols) * 2}개 특성 추가\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 파생 특성 생성 (모두 t-1, t-2 시점 기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. 파생 특성 생성\n",
      "================================================================================\n",
      "✓ 파생 특성 생성 완료\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. 파생 특성 생성\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 변화율 (t-1과 t-2 비교)\n",
    "df['매출_변화율'] = (df['당월_매출_금액_lag1'] - df['당월_매출_금액_lag2']) / (df['당월_매출_금액_lag2'] + 1)\n",
    "df['매출건수_변화율'] = (df['당월_매출_건수_lag1'] - df['당월_매출_건수_lag2']) / (df['당월_매출_건수_lag2'] + 1)\n",
    "df['점포수_변화율'] = (df['점포_수_lag1'] - df['점포_수_lag2']) / (df['점포_수_lag2'] + 1)\n",
    "df['개업률_변화'] = df['개업_률_lag1'] - df['개업_률_lag2']\n",
    "\n",
    "# 연속 하락 (t-2, t-1 시점 기준)\n",
    "df['매출_감소'] = (df['매출_변화율'] < 0).astype(int)\n",
    "df['연속_매출_감소'] = df.groupby(['자치구_encoded', '업종_encoded'])['매출_감소'].shift(1).rolling(2, min_periods=1).sum()\n",
    "\n",
    "# 과거 추세\n",
    "df['매출_3분기_평균'] = df.groupby(['자치구_encoded', '업종_encoded'])['당월_매출_금액_lag1'].shift(1).rolling(3, min_periods=1).mean()\n",
    "df['매출_추세_대비'] = df['당월_매출_금액_lag1'] / (df['매출_3분기_평균'] + 1)\n",
    "\n",
    "# 수익성 지표 (t-1 시점)\n",
    "df['점포당_매출'] = df['당월_매출_금액_lag1'] / (df['점포_수_lag1'] + 1)\n",
    "df['건당_매출'] = df['당월_매출_금액_lag1'] / (df['당월_매출_건수_lag1'] + 1)\n",
    "df['임대료_부담률'] = df['전체임대료_lag1'] / (df['당월_매출_금액_lag1'] + 1)\n",
    "df['유동인구_전환율'] = df['당월_매출_건수_lag1'] / (df['총_유동인구_수_lag1'] + 1)\n",
    "\n",
    "# 고객 구조 (t-1 시점)\n",
    "age_cols_lag1 = [f'연령대_{age}_매출_금액_lag1' for age in ['10', '20', '30', '40', '50', '60_이상']]\n",
    "df['최대_연령대_비중'] = df[age_cols_lag1].max(axis=1) / (df['당월_매출_금액_lag1'] + 1)\n",
    "df['주말_매출_비율'] = (df['토요일_매출_금액_lag1'] + df['일요일_매출_금액_lag1']) / (df['당월_매출_금액_lag1'] + 1)\n",
    "df['야간_매출_비율'] = df['시간대_21_24_매출_금액_lag1'] / (df['당월_매출_금액_lag1'] + 1)\n",
    "\n",
    "# 경쟁 환경 (t-1 시점)\n",
    "df['프랜차이즈_비율'] = df['프랜차이즈_점포_수_lag1'] / (df['점포_수_lag1'] + 1)\n",
    "df['경쟁_밀도'] = df['유사_업종_점포_수_lag1'] / (df['점포_수_lag1'] + 1)\n",
    "df['점포_포화도'] = df['점포_수_lag1'] / ((df['총_상주인구_수_lag1'] + df['총_직장인구_수_lag1']) + 1) * 10000\n",
    "\n",
    "print(f\"✓ 파생 특성 생성 완료\")\n",
    "\n",
    "# 결측값 처리\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature 선택\n",
    "\n",
    "**핵심:**\n",
    "- 현재 시점(t)의 모든 원본 데이터 제외\n",
    "- 오직 lag 특성(_lag1, _lag2)만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. Feature 선택\n",
      "================================================================================\n",
      "✓ 전체 컬럼 수: 202\n",
      "✓ 제외 컬럼 수: 141\n",
      "✓ 사용 feature 수: 61\n",
      "✓ 범주형 feature: 3개\n",
      "  - cat_idxs: [0, 1, 2]\n",
      "  - cat_dims: [25, 63, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. Feature 선택\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 범주형 변수\n",
    "categorical_features = ['자치구_encoded', '업종_encoded', '상권변화_encoded']\n",
    "\n",
    "# 제외할 컬럼들\n",
    "exclude_cols = [\n",
    "    # 원본 범주형 변수\n",
    "    '기준_년분기_코드', '자치구_코드_명', '서비스_업종_코드_명', '상권_변화_지표',\n",
    "\n",
    "    # 🚫 타겟 변수 및 현재 시점(t) 모든 데이터\n",
    "    '폐업_률', '폐업_점포_수', 'closure_risk',\n",
    "\n",
    "    # 🚫 현재 시점(t)의 모든 원본 데이터 (lag가 없는 컬럼들)\n",
    "    '당월_매출_금액', '당월_매출_건수', '점포_수', '유사_업종_점포_수',\n",
    "    '프랜차이즈_점포_수', '개업_률', '개업_점포_수',\n",
    "    '전체임대료', '총_유동인구_수', '총_상주인구_수', '총_직장인구_수',\n",
    "    '토요일_매출_금액', '일요일_매출_금액', '시간대_21_24_매출_금액',\n",
    "    '연령대_10_매출_금액', '연령대_20_매출_금액', '연령대_30_매출_금액',\n",
    "    '연령대_40_매출_금액', '연령대_50_매출_금액', '연령대_60_이상_매출_금액',\n",
    "\n",
    "    # 시간 정보\n",
    "    'year', 'quarter', 'year_quarter',\n",
    "\n",
    "    # 기타 현재 시점 데이터 (모두 제외)\n",
    "    '월요일_매출_금액', '화요일_매출_금액', '수요일_매출_금액', '목요일_매출_금액', '금요일_매출_금액',\n",
    "    '시간대_00_06_매출_금액', '시간대_06_11_매출_금액', '시간대_11_14_매출_금액',\n",
    "    '시간대_14_17_매출_금액', '시간대_17_21_매출_금액',\n",
    "    '남성_매출_금액', '여성_매출_금액',\n",
    "    '월요일_매출_건수', '화요일_매출_건수', '수요일_매출_건수', '목요일_매출_건수',\n",
    "    '금요일_매출_건수', '토요일_매출_건수', '일요일_매출_건수',\n",
    "    '시간대_건수~06_매출_건수', '시간대_건수~11_매출_건수', '시간대_건수~14_매출_건수',\n",
    "    '시간대_건수~17_매출_건수', '시간대_건수~21_매출_건수', '시간대_건수~24_매출_건수',\n",
    "    '남성_매출_건수', '여성_매출_건수',\n",
    "    '연령대_10_매출_건수', '연령대_20_매출_건수', '연령대_30_매출_건수',\n",
    "    '연령대_40_매출_건수', '연령대_50_매출_건수', '연령대_60_이상_매출_건수',\n",
    "    '운영_영업_개월_평균', '폐업_영업_개월_평균', '서울시_운영_영업_개월_평균', '서울시_폐업_영업_개월_평균',\n",
    "    '남성_유동인구_수', '여성_유동인구_수',\n",
    "    '연령대_10_유동인구_수', '연령대_20_유동인구_수', '연령대_30_유동인구_수',\n",
    "    '연령대_40_유동인구_수', '연령대_50_유동인구_수', '연령대_60_이상_유동인구_수',\n",
    "    '시간대_00_06_유동인구_수', '시간대_06_11_유동인구_수', '시간대_11_14_유동인구_수',\n",
    "    '시간대_14_17_유동인구_수', '시간대_17_21_유동인구_수', '시간대_21_24_유동인구_수',\n",
    "    '월요일_유동인구_수', '화요일_유동인구_수', '수요일_유동인구_수', '목요일_유동인구_수',\n",
    "    '금요일_유동인구_수', '토요일_유동인구_수', '일요일_유동인구_수',\n",
    "    '월_평균_소득_금액', '지출_총_금액', '식료품_지출_총금액', '의류_신발_지출_총금액',\n",
    "    '생활용품_지출_총금액', '의료비_지출_총금액', '교통_지출_총금액', '교육_지출_총금액',\n",
    "    '유흥_지출_총금액', '여가_문화_지출_총금액', '기타_지출_총금액', '음식_지출_총금액',\n",
    "    '남성_상주인구_수', '여성_상주인구_수',\n",
    "    '연령대_10_미만_상주인구_수', '연령대_20_상주인구_수', '연령대_30_상주인구_수',\n",
    "    '연령대_40_상주인구_수', '연령대_50_상주인구_수', '연령대_60_이상_상주인구_수',\n",
    "    '남성연령대_10_상주인구_수', '남성연령대_20_상주인구_수', '남성연령대_30_상주인구_수',\n",
    "    '남성연령대_40_상주인구_수', '남성연령대_50_상주인구_수', '남성연령대_60_이상_상주인구_수',\n",
    "    '여성연령대_10_상주인구_수', '여성연령대_20_상주인구_수', '여성연령대_30_상주인구_수',\n",
    "    '여성연령대_40_상주인구_수', '여성연령대_50_상주인구_수', '여성연령대_60_이상_상주인구_수',\n",
    "    '총_가구_수', '남성_직장인구_수', '여성_직장인구_수',\n",
    "    '연령대_10_직장인구_수', '연령대_20_직장인구_수', '연령대_30_직장인구_수',\n",
    "    '연령대_40_직장인구_수', '연령대_50_직장인구_수', '연령대_60_이상_직장인구_수',\n",
    "    '남성연령대_10_직장_인구_수', '남성연령대_20_직장_인구_수', '남성연령대_30_직장_인구_수',\n",
    "    '남성연령대_40_직장_인구_수', '남성연령대_50_직장_인구_수', '남성연령대_60_이상_직장_인구_수',\n",
    "    '여성연령대_10_직장_인구_수', '여성연령대_20_직장_인구_수', '여성연령대_30_직장_인구_수',\n",
    "    '여성연령대_40_직장_인구_수', '여성연령대_50_직장_인구_수', '여성연령대_60_이상_직장_인구_수'\n",
    "]\n",
    "\n",
    "# 사용 가능한 feature 선택\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# 범주형 feature의 인덱스 및 차원 설정\n",
    "cat_idxs = [feature_cols.index(col) for col in categorical_features if col in feature_cols]\n",
    "cat_dims = [\n",
    "    len(le_district.classes_),\n",
    "    len(le_industry.classes_),\n",
    "    len(le_change.classes_)\n",
    "]\n",
    "\n",
    "print(f\"✓ 전체 컬럼 수: {len(df.columns)}\")\n",
    "print(f\"✓ 제외 컬럼 수: {len(exclude_cols)}\")\n",
    "print(f\"✓ 사용 feature 수: {len(feature_cols)}\")\n",
    "print(f\"✓ 범주형 feature: {len(categorical_features)}개\")\n",
    "print(f\"  - cat_idxs: {cat_idxs}\")\n",
    "print(f\"  - cat_dims: {cat_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 분할 (시계열 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "6. 데이터 분할 - 시계열 기준\n",
      "================================================================================\n",
      "✓ 초기 분기 제거 후 Shape: (36875, 202)\n",
      "✓ Train: 25812 (70.0%)\n",
      "  - Class 0: 12938, Class 1: 12874\n",
      "✓ Validation: 3687 (10.0%)\n",
      "  - Class 0: 1705, Class 1: 1982\n",
      "✓ Test: 7376 (20.0%)\n",
      "  - Class 0: 3568, Class 1: 3808\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. 데이터 분할 - 시계열 기준\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. 결측값 제거\n",
    "df_clean = df.dropna(subset=feature_cols).reset_index(drop=True)\n",
    "\n",
    "# 2. 각 그룹의 초기 데이터 제거 (lag2 생성 이후부터 사용)\n",
    "# lag2 사용할 경우 처음 2분기까지는 데이터가 모두 0으로 처리됨\n",
    "df_clean = df_clean.groupby(['자치구_encoded', '업종_encoded'], group_keys=False).apply(\n",
    "    lambda x: x.iloc[2:] if len(x) > 2 else x  # 각 그룹에서 처음 2개 분기 제거\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 3. 시계열 정렬\n",
    "df_sorted = df_clean.sort_values(\n",
    "    ['year_quarter', '자치구_encoded', '업종_encoded']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ 초기 분기 제거 후 Shape: {df_sorted.shape}\")\n",
    "\n",
    "# Train/Val/Test 분할\n",
    "train_size = 0.7\n",
    "val_size = 0.1\n",
    "test_size = 0.2\n",
    "\n",
    "n_total = len(df_sorted)\n",
    "n_train = int(n_total * train_size)\n",
    "n_val = int(n_total * val_size)\n",
    "\n",
    "train_df = df_sorted.iloc[:n_train].copy()\n",
    "val_df = df_sorted.iloc[n_train:n_train+n_val].copy()\n",
    "test_df = df_sorted.iloc[n_train+n_val:].copy()\n",
    "\n",
    "print(f\"✓ Train: {len(train_df)} ({len(train_df)/n_total*100:.1f}%)\")\n",
    "print(f\"  - Class 0: {(train_df['closure_risk']==0).sum()}, Class 1: {(train_df['closure_risk']==1).sum()}\")\n",
    "print(f\"✓ Validation: {len(val_df)} ({len(val_df)/n_total*100:.1f}%)\")\n",
    "print(f\"  - Class 0: {(val_df['closure_risk']==0).sum()}, Class 1: {(val_df['closure_risk']==1).sum()}\")\n",
    "print(f\"✓ Test: {len(test_df)} ({len(test_df)/n_total*100:.1f}%)\")\n",
    "print(f\"  - Class 0: {(test_df['closure_risk']==0).sum()}, Class 1: {(test_df['closure_risk']==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 그룹 통계량 계산 - Train 데이터만 사용 (Leakage 방지)\n",
    "\n",
    "**핵심:**\n",
    "- Train에서만 그룹 평균 계산\n",
    "- Val/Test는 Train의 통계량 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "7. 그룹 통계량 계산 - Train 데이터만 사용 (Leakage 방지)\n",
      "================================================================================\n",
      "✓ 최종 feature 수: 66\n",
      "✓ 그룹 통계량 기반 특성 추가: 5개\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. 그룹 통계량 계산 - Train 데이터만 사용 (Leakage 방지)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train에서만 그룹 평균 계산\n",
    "district_sales_mean = train_df.groupby('자치구_encoded')['당월_매출_금액_lag1'].mean()\n",
    "industry_sales_mean = train_df.groupby('업종_encoded')['당월_매출_금액_lag1'].mean()\n",
    "\n",
    "# 모든 세트에 적용 (train의 통계량 사용)\n",
    "train_df['상권_매출_평균'] = train_df['자치구_encoded'].map(district_sales_mean)\n",
    "train_df['업종_매출_평균'] = train_df['업종_encoded'].map(industry_sales_mean)\n",
    "\n",
    "val_df['상권_매출_평균'] = val_df['자치구_encoded'].map(district_sales_mean)\n",
    "val_df['업종_매출_평균'] = val_df['업종_encoded'].map(industry_sales_mean)\n",
    "\n",
    "test_df['상권_매출_평균'] = test_df['자치구_encoded'].map(district_sales_mean)\n",
    "test_df['업종_매출_평균'] = test_df['업종_encoded'].map(industry_sales_mean)\n",
    "\n",
    "# 상대 지표 계산\n",
    "for dataset in [train_df, val_df, test_df]:\n",
    "    dataset['상권_대비_매출'] = dataset['당월_매출_금액_lag1'] / (dataset['상권_매출_평균'] + 1)\n",
    "    dataset['업종_대비_매출'] = dataset['당월_매출_금액_lag1'] / (dataset['업종_매출_평균'] + 1)\n",
    "\n",
    "    # 위험 점수 (t-1 시점 데이터 기반)\n",
    "    dataset['위험_점수'] = (\n",
    "        (dataset['매출_변화율'] < -0.1).astype(int) * 2 +\n",
    "        (dataset['임대료_부담률'] > 0.3).astype(int) * 2 +\n",
    "        (dataset['상권_대비_매출'] < 0.7).astype(int) +\n",
    "        (dataset.get('연속_매출_감소', 0) >= 2).astype(int) * 3\n",
    "    )\n",
    "\n",
    "# Feature 리스트 업데이트\n",
    "additional_features = ['상권_매출_평균', '업종_매출_평균', '상권_대비_매출', '업종_대비_매출', '위험_점수']\n",
    "feature_cols.extend(additional_features)\n",
    "\n",
    "# 범주형 인덱스 업데이트\n",
    "cat_idxs = [feature_cols.index(col) for col in categorical_features if col in feature_cols]\n",
    "\n",
    "print(f\"✓ 최종 feature 수: {len(feature_cols)}\")\n",
    "print(f\"✓ 그룹 통계량 기반 특성 추가: {len(additional_features)}개\")\n",
    "\n",
    "# 결측값 처리\n",
    "train_df = train_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "val_df = val_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_df = test_df.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### 데이터 저장 - 공유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# train_df, val_df, test_df에서 feature_cols만 추출해서 저장\n",
    "train_features = train_df[feature_cols]\n",
    "train_target = train_df['closure_risk']\n",
    "\n",
    "val_features = val_df[feature_cols]\n",
    "val_target = val_df['closure_risk']\n",
    "\n",
    "test_features = test_df[feature_cols]\n",
    "test_target = test_df['closure_risk']\n",
    "\n",
    "# 저장.1 (validation set 분할 필요한 경우) - 파일 경로 수정 필요\n",
    "train_features.to_csv('./preprocessed/train_features.csv', index=False)\n",
    "train_target.to_csv('./preprocessed/train_target.csv', index=False)\n",
    "val_features.to_csv('./preprocessed/val_features.csv', index=False)\n",
    "val_target.to_csv('./preprocessed/val_target.csv', index=False)\n",
    "test_features.to_csv('./preprocessed/test_features.csv', index=False)\n",
    "test_target.to_csv('./preprocessed/test_target.csv', index=False)\n",
    "\n",
    "# 저장.2 (validation set 따로 분할 필요 없는 경우)\n",
    "# train_val_features = pd.concat([train_features, val_features], axis=0)\n",
    "# train_val_target = pd.concat([train_target, val_target], axis=0)\n",
    "# train_val_features.to_csv('./preprocessed/train_val.csv', index=False)\n",
    "# train_val_target.to_csv('./preprocessed/train_target.csv', index=False)\n",
    "# test_features.to_csv('./preprocessed/test_features.csv', index=False)\n",
    "# test_target.to_csv('./preprocessed/test_target.csv', index=False)\n",
    "\n",
    "print('데이터 저장 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 데이터 준비 및 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "8. 데이터 준비 및 스케일링\n",
      "================================================================================\n",
      "✓ 연속형 feature 수: 63개\n",
      "✓ 범주형 feature 수: 3개\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. 데이터 준비 및 스케일링\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 데이터 추출\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['closure_risk'].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['closure_risk'].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['closure_risk'].values\n",
    "\n",
    "# 범주형과 연속형 분리\n",
    "categorical_mask = np.zeros(len(feature_cols), dtype=bool)\n",
    "for idx in cat_idxs:\n",
    "    categorical_mask[idx] = True\n",
    "continuous_mask = ~categorical_mask\n",
    "\n",
    "# Feature 스케일링 (연속형 변수만)\n",
    "scaler = StandardScaler()\n",
    "X_train_continuous = X_train[:, continuous_mask].astype(np.float64)\n",
    "X_val_continuous = X_val[:, continuous_mask].astype(np.float64)\n",
    "X_test_continuous = X_test[:, continuous_mask].astype(np.float64)\n",
    "\n",
    "X_train_continuous_scaled = scaler.fit_transform(X_train_continuous)\n",
    "X_val_continuous_scaled = scaler.transform(X_val_continuous)\n",
    "X_test_continuous_scaled = scaler.transform(X_test_continuous)\n",
    "\n",
    "# 범주형과 스케일링된 연속형 결합\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[:, continuous_mask] = X_train_continuous_scaled\n",
    "X_val_scaled[:, continuous_mask] = X_val_continuous_scaled\n",
    "X_test_scaled[:, continuous_mask] = X_test_continuous_scaled\n",
    "\n",
    "# MPS 호환성 \n",
    "if device == 'mps':\n",
    "    X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "    X_val_scaled = X_val_scaled.astype(np.float32)\n",
    "    X_test_scaled = X_test_scaled.astype(np.float32)\n",
    "\n",
    "print(f\"✓ 연속형 feature 수: {continuous_mask.sum()}개\")\n",
    "print(f\"✓ 범주형 feature 수: {categorical_mask.sum()}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TabNet 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "9. TabNet 모델 학습\n",
      "================================================================================\n",
      "\n",
      "✓ 모델 학습 시작\n",
      "\n",
      "epoch 0  | loss: 0.75571 | val_0_accuracy: 0.61242 | val_0_auc: 0.65446 | val_0_logloss: 0.67075 |  0:00:09s\n",
      "epoch 1  | loss: 0.67328 | val_0_accuracy: 0.65989 | val_0_auc: 0.71916 | val_0_logloss: 0.643   |  0:00:13s\n",
      "epoch 2  | loss: 0.61778 | val_0_accuracy: 0.67426 | val_0_auc: 0.75827 | val_0_logloss: 0.60785 |  0:00:17s\n",
      "epoch 3  | loss: 0.589   | val_0_accuracy: 0.69243 | val_0_auc: 0.77229 | val_0_logloss: 0.57571 |  0:00:22s\n",
      "epoch 4  | loss: 0.58014 | val_0_accuracy: 0.69731 | val_0_auc: 0.7779  | val_0_logloss: 0.57062 |  0:00:26s\n",
      "epoch 5  | loss: 0.58336 | val_0_accuracy: 0.69026 | val_0_auc: 0.77348 | val_0_logloss: 0.56576 |  0:00:30s\n",
      "epoch 6  | loss: 0.57852 | val_0_accuracy: 0.69406 | val_0_auc: 0.77818 | val_0_logloss: 0.56393 |  0:00:34s\n",
      "epoch 7  | loss: 0.57034 | val_0_accuracy: 0.69053 | val_0_auc: 0.78459 | val_0_logloss: 0.56442 |  0:00:39s\n",
      "epoch 8  | loss: 0.56697 | val_0_accuracy: 0.69325 | val_0_auc: 0.78464 | val_0_logloss: 0.55819 |  0:00:43s\n",
      "epoch 9  | loss: 0.56602 | val_0_accuracy: 0.69081 | val_0_auc: 0.78766 | val_0_logloss: 0.56005 |  0:00:47s\n",
      "epoch 10 | loss: 0.56707 | val_0_accuracy: 0.6927  | val_0_auc: 0.78289 | val_0_logloss: 0.55705 |  0:00:51s\n",
      "epoch 11 | loss: 0.56666 | val_0_accuracy: 0.70057 | val_0_auc: 0.78734 | val_0_logloss: 0.55207 |  0:00:56s\n",
      "epoch 12 | loss: 0.56352 | val_0_accuracy: 0.69406 | val_0_auc: 0.78801 | val_0_logloss: 0.55422 |  0:01:00s\n",
      "epoch 13 | loss: 0.55932 | val_0_accuracy: 0.69243 | val_0_auc: 0.78868 | val_0_logloss: 0.55653 |  0:01:04s\n",
      "epoch 14 | loss: 0.55704 | val_0_accuracy: 0.70464 | val_0_auc: 0.7924  | val_0_logloss: 0.54832 |  0:01:08s\n",
      "epoch 15 | loss: 0.55767 | val_0_accuracy: 0.69731 | val_0_auc: 0.79175 | val_0_logloss: 0.55095 |  0:01:12s\n",
      "epoch 16 | loss: 0.55969 | val_0_accuracy: 0.69596 | val_0_auc: 0.78841 | val_0_logloss: 0.5527  |  0:01:17s\n",
      "epoch 17 | loss: 0.55855 | val_0_accuracy: 0.69596 | val_0_auc: 0.79036 | val_0_logloss: 0.55448 |  0:01:21s\n",
      "epoch 18 | loss: 0.55214 | val_0_accuracy: 0.70328 | val_0_auc: 0.79207 | val_0_logloss: 0.5486  |  0:01:25s\n",
      "epoch 19 | loss: 0.55243 | val_0_accuracy: 0.70681 | val_0_auc: 0.79267 | val_0_logloss: 0.54589 |  0:01:30s\n",
      "epoch 20 | loss: 0.5524  | val_0_accuracy: 0.70274 | val_0_auc: 0.79336 | val_0_logloss: 0.54782 |  0:01:34s\n",
      "epoch 21 | loss: 0.5528  | val_0_accuracy: 0.69433 | val_0_auc: 0.79222 | val_0_logloss: 0.55546 |  0:01:38s\n",
      "epoch 22 | loss: 0.55008 | val_0_accuracy: 0.70545 | val_0_auc: 0.79458 | val_0_logloss: 0.5468  |  0:01:42s\n",
      "epoch 23 | loss: 0.5479  | val_0_accuracy: 0.69976 | val_0_auc: 0.79653 | val_0_logloss: 0.55028 |  0:01:46s\n",
      "epoch 24 | loss: 0.54514 | val_0_accuracy: 0.71169 | val_0_auc: 0.79871 | val_0_logloss: 0.54105 |  0:01:50s\n",
      "epoch 25 | loss: 0.54423 | val_0_accuracy: 0.69894 | val_0_auc: 0.79953 | val_0_logloss: 0.54583 |  0:01:55s\n",
      "epoch 26 | loss: 0.54152 | val_0_accuracy: 0.71359 | val_0_auc: 0.80061 | val_0_logloss: 0.54202 |  0:01:59s\n",
      "epoch 27 | loss: 0.54015 | val_0_accuracy: 0.7144  | val_0_auc: 0.79993 | val_0_logloss: 0.54022 |  0:02:03s\n",
      "epoch 28 | loss: 0.53873 | val_0_accuracy: 0.7163  | val_0_auc: 0.80197 | val_0_logloss: 0.537   |  0:02:07s\n",
      "epoch 29 | loss: 0.53686 | val_0_accuracy: 0.71657 | val_0_auc: 0.80108 | val_0_logloss: 0.53706 |  0:02:12s\n",
      "epoch 30 | loss: 0.53629 | val_0_accuracy: 0.71006 | val_0_auc: 0.80256 | val_0_logloss: 0.54055 |  0:02:16s\n",
      "epoch 31 | loss: 0.53286 | val_0_accuracy: 0.71711 | val_0_auc: 0.80243 | val_0_logloss: 0.53963 |  0:02:20s\n",
      "epoch 32 | loss: 0.53307 | val_0_accuracy: 0.71549 | val_0_auc: 0.80164 | val_0_logloss: 0.54372 |  0:02:24s\n",
      "epoch 33 | loss: 0.53073 | val_0_accuracy: 0.71928 | val_0_auc: 0.80162 | val_0_logloss: 0.53729 |  0:02:29s\n",
      "epoch 34 | loss: 0.53045 | val_0_accuracy: 0.72281 | val_0_auc: 0.80372 | val_0_logloss: 0.53731 |  0:02:33s\n",
      "epoch 35 | loss: 0.53024 | val_0_accuracy: 0.72172 | val_0_auc: 0.80359 | val_0_logloss: 0.53529 |  0:02:37s\n",
      "epoch 36 | loss: 0.53052 | val_0_accuracy: 0.71901 | val_0_auc: 0.80432 | val_0_logloss: 0.53959 |  0:02:41s\n",
      "epoch 37 | loss: 0.52759 | val_0_accuracy: 0.72335 | val_0_auc: 0.80394 | val_0_logloss: 0.53679 |  0:02:46s\n",
      "epoch 38 | loss: 0.528   | val_0_accuracy: 0.72118 | val_0_auc: 0.80427 | val_0_logloss: 0.53441 |  0:02:50s\n",
      "epoch 39 | loss: 0.52717 | val_0_accuracy: 0.72118 | val_0_auc: 0.80514 | val_0_logloss: 0.53473 |  0:02:54s\n",
      "epoch 40 | loss: 0.5277  | val_0_accuracy: 0.72417 | val_0_auc: 0.80915 | val_0_logloss: 0.53183 |  0:02:59s\n",
      "epoch 41 | loss: 0.52575 | val_0_accuracy: 0.72634 | val_0_auc: 0.8072  | val_0_logloss: 0.53036 |  0:03:04s\n",
      "epoch 42 | loss: 0.52456 | val_0_accuracy: 0.73474 | val_0_auc: 0.81061 | val_0_logloss: 0.52518 |  0:03:08s\n",
      "epoch 43 | loss: 0.52424 | val_0_accuracy: 0.72498 | val_0_auc: 0.8072  | val_0_logloss: 0.53017 |  0:03:12s\n",
      "epoch 44 | loss: 0.52308 | val_0_accuracy: 0.72742 | val_0_auc: 0.80794 | val_0_logloss: 0.53451 |  0:03:16s\n",
      "epoch 45 | loss: 0.52322 | val_0_accuracy: 0.72851 | val_0_auc: 0.81043 | val_0_logloss: 0.5282  |  0:03:21s\n",
      "epoch 46 | loss: 0.52145 | val_0_accuracy: 0.72688 | val_0_auc: 0.80882 | val_0_logloss: 0.53505 |  0:03:25s\n",
      "epoch 47 | loss: 0.51922 | val_0_accuracy: 0.73176 | val_0_auc: 0.81188 | val_0_logloss: 0.52482 |  0:03:29s\n",
      "epoch 48 | loss: 0.51865 | val_0_accuracy: 0.72688 | val_0_auc: 0.80875 | val_0_logloss: 0.52981 |  0:03:33s\n",
      "epoch 49 | loss: 0.51825 | val_0_accuracy: 0.71956 | val_0_auc: 0.80909 | val_0_logloss: 0.53389 |  0:03:38s\n",
      "epoch 50 | loss: 0.51773 | val_0_accuracy: 0.72796 | val_0_auc: 0.81046 | val_0_logloss: 0.52824 |  0:03:42s\n",
      "epoch 51 | loss: 0.51767 | val_0_accuracy: 0.72335 | val_0_auc: 0.81092 | val_0_logloss: 0.52921 |  0:03:46s\n",
      "epoch 52 | loss: 0.51681 | val_0_accuracy: 0.72634 | val_0_auc: 0.80894 | val_0_logloss: 0.52985 |  0:03:51s\n",
      "epoch 53 | loss: 0.51679 | val_0_accuracy: 0.72715 | val_0_auc: 0.8096  | val_0_logloss: 0.52816 |  0:03:55s\n",
      "epoch 54 | loss: 0.51625 | val_0_accuracy: 0.71983 | val_0_auc: 0.80815 | val_0_logloss: 0.53281 |  0:04:00s\n",
      "epoch 55 | loss: 0.51498 | val_0_accuracy: 0.72335 | val_0_auc: 0.8094  | val_0_logloss: 0.52993 |  0:04:04s\n",
      "epoch 56 | loss: 0.51587 | val_0_accuracy: 0.72823 | val_0_auc: 0.80969 | val_0_logloss: 0.52569 |  0:04:08s\n",
      "epoch 57 | loss: 0.51584 | val_0_accuracy: 0.72145 | val_0_auc: 0.80782 | val_0_logloss: 0.531   |  0:04:14s\n",
      "epoch 58 | loss: 0.51482 | val_0_accuracy: 0.71711 | val_0_auc: 0.80766 | val_0_logloss: 0.53495 |  0:04:18s\n",
      "epoch 59 | loss: 0.51423 | val_0_accuracy: 0.72091 | val_0_auc: 0.80743 | val_0_logloss: 0.53284 |  0:04:23s\n",
      "epoch 60 | loss: 0.51422 | val_0_accuracy: 0.72579 | val_0_auc: 0.80769 | val_0_logloss: 0.53246 |  0:04:28s\n",
      "epoch 61 | loss: 0.51537 | val_0_accuracy: 0.72389 | val_0_auc: 0.80866 | val_0_logloss: 0.53203 |  0:04:32s\n",
      "epoch 62 | loss: 0.51297 | val_0_accuracy: 0.72118 | val_0_auc: 0.8077  | val_0_logloss: 0.53398 |  0:04:36s\n",
      "epoch 63 | loss: 0.51212 | val_0_accuracy: 0.72254 | val_0_auc: 0.80985 | val_0_logloss: 0.53163 |  0:04:41s\n",
      "epoch 64 | loss: 0.5106  | val_0_accuracy: 0.72471 | val_0_auc: 0.80795 | val_0_logloss: 0.5325  |  0:04:45s\n",
      "epoch 65 | loss: 0.51084 | val_0_accuracy: 0.72552 | val_0_auc: 0.81142 | val_0_logloss: 0.53104 |  0:04:49s\n",
      "epoch 66 | loss: 0.50973 | val_0_accuracy: 0.722   | val_0_auc: 0.80738 | val_0_logloss: 0.52968 |  0:04:53s\n",
      "epoch 67 | loss: 0.5108  | val_0_accuracy: 0.72715 | val_0_auc: 0.80661 | val_0_logloss: 0.535   |  0:04:58s\n",
      "epoch 68 | loss: 0.50983 | val_0_accuracy: 0.72688 | val_0_auc: 0.80917 | val_0_logloss: 0.53046 |  0:05:02s\n",
      "epoch 69 | loss: 0.50855 | val_0_accuracy: 0.72769 | val_0_auc: 0.80877 | val_0_logloss: 0.52622 |  0:05:06s\n",
      "epoch 70 | loss: 0.50818 | val_0_accuracy: 0.72389 | val_0_auc: 0.80936 | val_0_logloss: 0.52928 |  0:05:10s\n",
      "epoch 71 | loss: 0.50948 | val_0_accuracy: 0.722   | val_0_auc: 0.81054 | val_0_logloss: 0.53024 |  0:05:15s\n",
      "epoch 72 | loss: 0.50768 | val_0_accuracy: 0.72715 | val_0_auc: 0.81145 | val_0_logloss: 0.52724 |  0:05:19s\n",
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 47 and best_val_0_logloss = 0.52482\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7dcac0f398be>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✓ 모델 학습 시작\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_importance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# compute feature importance once the best model is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_compute_feature_importances\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \"\"\"\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mM_explain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0msum_explain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_explain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mfeature_importances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_explain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_explain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, X, normalize)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mM_explain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. TabNet 모델 학습\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = TabNetClassifier(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=3,\n",
    "    n_d=8,\n",
    "    n_a=8,\n",
    "    n_steps=3,\n",
    "    gamma=1.4,\n",
    "    lambda_sparse=0.000009,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.01),\n",
    "    scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    seed=SEED,\n",
    "    verbose=1,\n",
    "    device_name=device,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ 모델 학습 시작\\n\")\n",
    "model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    eval_metric=['accuracy', 'auc', 'logloss'],\n",
    "    max_epochs=150,\n",
    "    patience=25,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=256\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ 학습 완료\")\n",
    "print(f\"  - Best Epoch: {model.best_epoch}\")\n",
    "print(f\"  - Best Loss: {model.best_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모델 평가 - Data Leakage 제거 후 실제 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. 모델 평가\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 최적 임계값 찾기\n",
    "y_val_proba = model.predict_proba(X_val_scaled)\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_proba[:, 1])\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_threshold = roc_thresholds[optimal_idx]\n",
    "\n",
    "print(f\"✓ 최적 임계값: {optimal_threshold:.4f}\")\n",
    "\n",
    "# 테스트 세트 평가\n",
    "y_test_proba = model.predict_proba(X_test_scaled)\n",
    "y_test_pred = (y_test_proba[:, 1] >= optimal_threshold).astype(int)\n",
    "\n",
    "# 평가 지표 계산\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "auc = roc_auc_score(y_test, y_test_proba[:, 1])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'테스트 세트 최종 성능':^80}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  AUC:       {auc:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"\\n✓ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\n✓ Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Low Risk', 'High Risk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Low Risk', 'High Risk'],\n",
    "    yticklabels=['Low Risk', 'High Risk'],\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Confusion Matrix', fontsize=14, pad=20)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"11. Feature Importance (Top 20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Top 20) 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.barh(range(len(importance_df)), importance_df['importance'])\n",
    "ax.set_yticks(range(len(importance_df)))\n",
    "ax.set_yticklabels(importance_df['feature'], fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Top 20 Feature Importance', fontsize=14, pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
