{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet íì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸ - Data Leakage ì™„ì „ ì œê±° ë²„ì „\n",
    "\n",
    "**ì£¼ìš” ìˆ˜ì •ì‚¬í•­:**\n",
    "1. íƒ€ê²Ÿ ë³€ìˆ˜: t ì‹œì  ë°ì´í„° â†’ t+1 ì‹œì  íì—… ìœ„í—˜ ì˜ˆì¸¡\n",
    "2. íì—…_ë¥ , íì—…_ì í¬_ìˆ˜ íŠ¹ì„± ì™„ì „ ì œê±°\n",
    "3. í˜„ì¬ ë¶„ê¸°(t) ë°ì´í„° ì‚¬ìš© ê¸ˆì§€ - ì˜¤ì§ ê³¼ê±°(t-1, t-2) ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "4. ê·¸ë£¹ í†µê³„ëŸ‰ì€ trainì—ì„œë§Œ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° GPU ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Mac GPU ì‚¬ìš©\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"âœ“ Mac GPU ì‚¬ìš©\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"âœ“ NVIDIA GPU ì‚¬ìš©\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"âœ“ CPU ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. ë°ì´í„° ë¡œë“œ\n",
      "================================================================================\n",
      "âœ“ ë°ì´í„° Shape: (39975, 137)\n",
      "âœ“ ê¸°ê°„: 20191 ~ 20252\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# merged_data.csv ìˆëŠ” í´ë” ê²½ë¡œ -> ìˆ˜ì • í•„ìš”\n",
    "data_path = '/Users/kimjm/Desktop/SKN_19/SKN19_2nd_1team/eda/data/merged_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"âœ“ ë°ì´í„° Shape: {df.shape}\")\n",
    "print(f\"âœ“ ê¸°ê°„: {df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].min()} ~ {df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë¯¸ë˜ ì‹œì  ì˜ˆì¸¡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±\n",
      "================================================================================\n",
      "âœ“ íì—…ë¥  ì„ê³„ê°’: 2.30%\n",
      "âœ“ í´ë˜ìŠ¤ ë¶„í¬:\n",
      "closure_risk\n",
      "1    20320\n",
      "0    19655\n",
      "Name: count, dtype: int64\n",
      "âœ“ íƒ€ê²Ÿ ìƒì„± í›„ ë°ì´í„° Shape: (39975, 144)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì‹œê°„ ë³€ìˆ˜ ìƒì„±\n",
    "df['year'] = df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(str).str[:4].astype(int)\n",
    "df['quarter'] = df['ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ'].astype(str).str[4:].astype(int)\n",
    "df['year_quarter'] = df['year'] * 10 + df['quarter']\n",
    "\n",
    "# ë²”ì£¼í˜• ì¸ì½”ë”©\n",
    "le_district = LabelEncoder()\n",
    "df['ìì¹˜êµ¬_encoded'] = le_district.fit_transform(df['ìì¹˜êµ¬_ì½”ë“œ_ëª…'])\n",
    "\n",
    "le_industry = LabelEncoder()\n",
    "df['ì—…ì¢…_encoded'] = le_industry.fit_transform(df['ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…'])\n",
    "\n",
    "le_change = LabelEncoder()\n",
    "df['ìƒê¶Œë³€í™”_encoded'] = le_change.fit_transform(df['ìƒê¶Œ_ë³€í™”_ì§€í‘œ'])\n",
    "\n",
    "# ì‹œê³„ì—´ ì •ë ¬\n",
    "df = df.sort_values(['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded', 'year_quarter']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ì „ì²´ ë°ì´í„°ì˜ ì¤‘ì•™ê°’ ê¸°ì¤€)\n",
    "threshold = df['íì—…_ë¥ '].quantile(0.5)\n",
    "df['closure_risk'] = (df['íì—…_ë¥ '] >= threshold).astype(int)\n",
    "\n",
    "print(f\"âœ“ íì—…ë¥  ì„ê³„ê°’: {threshold:.2f}%\")\n",
    "print(f\"âœ“ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "print(df['closure_risk'].value_counts())\n",
    "\n",
    "# íƒ€ê²Ÿì´ ì—†ëŠ” ë§ˆì§€ë§‰ ë¶„ê¸° ë°ì´í„° ì œê±°\n",
    "df = df.dropna(subset=['íì—…_ë¥ ']).reset_index(drop=True)\n",
    "print(f\"âœ“ íƒ€ê²Ÿ ìƒì„± í›„ ë°ì´í„° Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. íŠ¹ì„± ìƒì„± - ê³¼ê±° ì‹œì  ë°ì´í„°ë§Œ ì‚¬ìš© (Data Leakage ë°©ì§€)\n",
    "\n",
    "**ì¤‘ìš”:**\n",
    "- ğŸš« íì—…_ë¥ , íì—…_ì í¬_ìˆ˜ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "- âœ… ì˜¤ì§ t-1, t-2 ì‹œì  ë°ì´í„°ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. íŠ¹ì„± ìƒì„±\n",
      "================================================================================\n",
      "âœ“ Lag íŠ¹ì„± ìƒì„± ëŒ€ìƒ: 20ê°œ ì»¬ëŸ¼\n",
      "âœ“ Lag íŠ¹ì„± ìƒì„± ì™„ë£Œ: 40ê°œ íŠ¹ì„± ì¶”ê°€\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. íŠ¹ì„± ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ğŸš« íì—…_ë¥ , íì—…_ì í¬_ìˆ˜ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!\n",
    "lag_cols = [\n",
    "    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 'ì í¬_ìˆ˜', 'ìœ ì‚¬_ì—…ì¢…_ì í¬_ìˆ˜',\n",
    "    'í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜', 'ê°œì—…_ë¥ ', 'ê°œì—…_ì í¬_ìˆ˜',\n",
    "    'ì „ì²´ì„ëŒ€ë£Œ', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì´_ì§ì¥ì¸êµ¬_ìˆ˜',\n",
    "    'í† ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ì¼ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_21_24_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì—°ë ¹ëŒ€_40_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_50_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ë§¤ì¶œ_ê¸ˆì•¡'\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Lag íŠ¹ì„± ìƒì„± ëŒ€ìƒ: {len(lag_cols)}ê°œ ì»¬ëŸ¼\")\n",
    "\n",
    "# Lag íŠ¹ì„± ìƒì„± (t-1, t-2)\n",
    "for col in lag_cols:\n",
    "    df[f'{col}_lag1'] = df.groupby(['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded'])[col].shift(1)\n",
    "    df[f'{col}_lag2'] = df.groupby(['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded'])[col].shift(2)\n",
    "\n",
    "print(f\"âœ“ Lag íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(lag_cols) * 2}ê°œ íŠ¹ì„± ì¶”ê°€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. íŒŒìƒ íŠ¹ì„± ìƒì„± (ëª¨ë‘ t-1, t-2 ì‹œì  ê¸°ë°˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. íŒŒìƒ íŠ¹ì„± ìƒì„±\n",
      "================================================================================\n",
      "âœ“ íŒŒìƒ íŠ¹ì„± ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. íŒŒìƒ íŠ¹ì„± ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë³€í™”ìœ¨ (t-1ê³¼ t-2 ë¹„êµ)\n",
    "df['ë§¤ì¶œ_ë³€í™”ìœ¨'] = (df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] - df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag2']) / (df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag2'] + 1)\n",
    "df['ë§¤ì¶œê±´ìˆ˜_ë³€í™”ìœ¨'] = (df['ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜_lag1'] - df['ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜_lag2']) / (df['ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜_lag2'] + 1)\n",
    "df['ì í¬ìˆ˜_ë³€í™”ìœ¨'] = (df['ì í¬_ìˆ˜_lag1'] - df['ì í¬_ìˆ˜_lag2']) / (df['ì í¬_ìˆ˜_lag2'] + 1)\n",
    "df['ê°œì—…ë¥ _ë³€í™”'] = df['ê°œì—…_ë¥ _lag1'] - df['ê°œì—…_ë¥ _lag2']\n",
    "\n",
    "# ì—°ì† í•˜ë½ (t-2, t-1 ì‹œì  ê¸°ì¤€)\n",
    "df['ë§¤ì¶œ_ê°ì†Œ'] = (df['ë§¤ì¶œ_ë³€í™”ìœ¨'] < 0).astype(int)\n",
    "df['ì—°ì†_ë§¤ì¶œ_ê°ì†Œ'] = df.groupby(['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded'])['ë§¤ì¶œ_ê°ì†Œ'].shift(1).rolling(2, min_periods=1).sum()\n",
    "\n",
    "# ê³¼ê±° ì¶”ì„¸\n",
    "df['ë§¤ì¶œ_3ë¶„ê¸°_í‰ê· '] = df.groupby(['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded'])['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'].shift(1).rolling(3, min_periods=1).mean()\n",
    "df['ë§¤ì¶œ_ì¶”ì„¸_ëŒ€ë¹„'] = df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] / (df['ë§¤ì¶œ_3ë¶„ê¸°_í‰ê· '] + 1)\n",
    "\n",
    "# ìˆ˜ìµì„± ì§€í‘œ (t-1 ì‹œì )\n",
    "df['ì í¬ë‹¹_ë§¤ì¶œ'] = df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] / (df['ì í¬_ìˆ˜_lag1'] + 1)\n",
    "df['ê±´ë‹¹_ë§¤ì¶œ'] = df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] / (df['ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜_lag1'] + 1)\n",
    "df['ì„ëŒ€ë£Œ_ë¶€ë‹´ë¥ '] = df['ì „ì²´ì„ëŒ€ë£Œ_lag1'] / (df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] + 1)\n",
    "df['ìœ ë™ì¸êµ¬_ì „í™˜ìœ¨'] = df['ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜_lag1'] / (df['ì´_ìœ ë™ì¸êµ¬_ìˆ˜_lag1'] + 1)\n",
    "\n",
    "# ê³ ê° êµ¬ì¡° (t-1 ì‹œì )\n",
    "age_cols_lag1 = [f'ì—°ë ¹ëŒ€_{age}_ë§¤ì¶œ_ê¸ˆì•¡_lag1' for age in ['10', '20', '30', '40', '50', '60_ì´ìƒ']]\n",
    "df['ìµœëŒ€_ì—°ë ¹ëŒ€_ë¹„ì¤‘'] = df[age_cols_lag1].max(axis=1) / (df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] + 1)\n",
    "df['ì£¼ë§_ë§¤ì¶œ_ë¹„ìœ¨'] = (df['í† ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] + df['ì¼ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡_lag1']) / (df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] + 1)\n",
    "df['ì•¼ê°„_ë§¤ì¶œ_ë¹„ìœ¨'] = df['ì‹œê°„ëŒ€_21_24_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] / (df['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] + 1)\n",
    "\n",
    "# ê²½ìŸ í™˜ê²½ (t-1 ì‹œì )\n",
    "df['í”„ëœì°¨ì´ì¦ˆ_ë¹„ìœ¨'] = df['í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜_lag1'] / (df['ì í¬_ìˆ˜_lag1'] + 1)\n",
    "df['ê²½ìŸ_ë°€ë„'] = df['ìœ ì‚¬_ì—…ì¢…_ì í¬_ìˆ˜_lag1'] / (df['ì í¬_ìˆ˜_lag1'] + 1)\n",
    "df['ì í¬_í¬í™”ë„'] = df['ì í¬_ìˆ˜_lag1'] / ((df['ì´_ìƒì£¼ì¸êµ¬_ìˆ˜_lag1'] + df['ì´_ì§ì¥ì¸êµ¬_ìˆ˜_lag1']) + 1) * 10000\n",
    "\n",
    "print(f\"âœ“ íŒŒìƒ íŠ¹ì„± ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature ì„ íƒ\n",
    "\n",
    "**í•µì‹¬:**\n",
    "- í˜„ì¬ ì‹œì (t)ì˜ ëª¨ë“  ì›ë³¸ ë°ì´í„° ì œì™¸\n",
    "- ì˜¤ì§ lag íŠ¹ì„±(_lag1, _lag2)ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. Feature ì„ íƒ\n",
      "================================================================================\n",
      "âœ“ ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 202\n",
      "âœ“ ì œì™¸ ì»¬ëŸ¼ ìˆ˜: 141\n",
      "âœ“ ì‚¬ìš© feature ìˆ˜: 61\n",
      "âœ“ ë²”ì£¼í˜• feature: 3ê°œ\n",
      "  - cat_idxs: [0, 1, 2]\n",
      "  - cat_dims: [25, 63, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. Feature ì„ íƒ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜\n",
    "categorical_features = ['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded', 'ìƒê¶Œë³€í™”_encoded']\n",
    "\n",
    "# ì œì™¸í•  ì»¬ëŸ¼ë“¤\n",
    "exclude_cols = [\n",
    "    # ì›ë³¸ ë²”ì£¼í˜• ë³€ìˆ˜\n",
    "    'ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ', 'ìì¹˜êµ¬_ì½”ë“œ_ëª…', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…', 'ìƒê¶Œ_ë³€í™”_ì§€í‘œ',\n",
    "\n",
    "    # ğŸš« íƒ€ê²Ÿ ë³€ìˆ˜ ë° í˜„ì¬ ì‹œì (t) ëª¨ë“  ë°ì´í„°\n",
    "    'íì—…_ë¥ ', 'íì—…_ì í¬_ìˆ˜', 'closure_risk',\n",
    "\n",
    "    # ğŸš« í˜„ì¬ ì‹œì (t)ì˜ ëª¨ë“  ì›ë³¸ ë°ì´í„° (lagê°€ ì—†ëŠ” ì»¬ëŸ¼ë“¤)\n",
    "    'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 'ì í¬_ìˆ˜', 'ìœ ì‚¬_ì—…ì¢…_ì í¬_ìˆ˜',\n",
    "    'í”„ëœì°¨ì´ì¦ˆ_ì í¬_ìˆ˜', 'ê°œì—…_ë¥ ', 'ê°œì—…_ì í¬_ìˆ˜',\n",
    "    'ì „ì²´ì„ëŒ€ë£Œ', 'ì´_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì´_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì´_ì§ì¥ì¸êµ¬_ìˆ˜',\n",
    "    'í† ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ì¼ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_21_24_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì—°ë ¹ëŒ€_40_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_50_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "\n",
    "    # ì‹œê°„ ì •ë³´\n",
    "    'year', 'quarter', 'year_quarter',\n",
    "\n",
    "    # ê¸°íƒ€ í˜„ì¬ ì‹œì  ë°ì´í„° (ëª¨ë‘ ì œì™¸)\n",
    "    'ì›”ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'í™”ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ìˆ˜ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ëª©ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ê¸ˆìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì‹œê°„ëŒ€_00_06_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_06_11_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_11_14_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì‹œê°„ëŒ€_14_17_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_17_21_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ë‚¨ì„±_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—¬ì„±_ë§¤ì¶œ_ê¸ˆì•¡',\n",
    "    'ì›”ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'í™”ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ìˆ˜ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ëª©ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ê¸ˆìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'í† ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ì¼ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ì‹œê°„ëŒ€_ê±´ìˆ˜~06_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~11_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~14_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ì‹œê°„ëŒ€_ê±´ìˆ˜~17_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~21_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~24_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ë‚¨ì„±_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—¬ì„±_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_40_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_50_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ë§¤ì¶œ_ê±´ìˆ˜',\n",
    "    'ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'íì—…_ì˜ì—…_ê°œì›”_í‰ê· ', 'ì„œìš¸ì‹œ_ìš´ì˜_ì˜ì—…_ê°œì›”_í‰ê· ', 'ì„œìš¸ì‹œ_íì—…_ì˜ì—…_ê°œì›”_í‰ê· ',\n",
    "    'ë‚¨ì„±_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—¬ì„±_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_10_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_20_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_40_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_50_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ì‹œê°„ëŒ€_00_06_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì‹œê°„ëŒ€_06_11_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì‹œê°„ëŒ€_11_14_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ì‹œê°„ëŒ€_14_17_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì‹œê°„ëŒ€_17_21_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì‹œê°„ëŒ€_21_24_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ì›”ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜', 'í™”ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜', 'ìˆ˜ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜', 'ëª©ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ê¸ˆìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜', 'í† ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜', 'ì¼ìš”ì¼_ìœ ë™ì¸êµ¬_ìˆ˜',\n",
    "    'ì›”_í‰ê· _ì†Œë“_ê¸ˆì•¡', 'ì§€ì¶œ_ì´_ê¸ˆì•¡', 'ì‹ë£Œí’ˆ_ì§€ì¶œ_ì´ê¸ˆì•¡', 'ì˜ë¥˜_ì‹ ë°œ_ì§€ì¶œ_ì´ê¸ˆì•¡',\n",
    "    'ìƒí™œìš©í’ˆ_ì§€ì¶œ_ì´ê¸ˆì•¡', 'ì˜ë£Œë¹„_ì§€ì¶œ_ì´ê¸ˆì•¡', 'êµí†µ_ì§€ì¶œ_ì´ê¸ˆì•¡', 'êµìœ¡_ì§€ì¶œ_ì´ê¸ˆì•¡',\n",
    "    'ìœ í¥_ì§€ì¶œ_ì´ê¸ˆì•¡', 'ì—¬ê°€_ë¬¸í™”_ì§€ì¶œ_ì´ê¸ˆì•¡', 'ê¸°íƒ€_ì§€ì¶œ_ì´ê¸ˆì•¡', 'ìŒì‹_ì§€ì¶œ_ì´ê¸ˆì•¡',\n",
    "    'ë‚¨ì„±_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—¬ì„±_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_10_ë¯¸ë§Œ_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_20_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_40_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_50_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ë‚¨ì„±ì—°ë ¹ëŒ€_10_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_20_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_30_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ë‚¨ì„±ì—°ë ¹ëŒ€_40_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_50_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_60_ì´ìƒ_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ì—¬ì„±ì—°ë ¹ëŒ€_10_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_20_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_30_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ì—¬ì„±ì—°ë ¹ëŒ€_40_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_50_ìƒì£¼ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_60_ì´ìƒ_ìƒì£¼ì¸êµ¬_ìˆ˜',\n",
    "    'ì´_ê°€êµ¬_ìˆ˜', 'ë‚¨ì„±_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì—¬ì„±_ì§ì¥ì¸êµ¬_ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_10_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_20_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_30_ì§ì¥ì¸êµ¬_ìˆ˜',\n",
    "    'ì—°ë ¹ëŒ€_40_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_50_ì§ì¥ì¸êµ¬_ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ì§ì¥ì¸êµ¬_ìˆ˜',\n",
    "    'ë‚¨ì„±ì—°ë ¹ëŒ€_10_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_20_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_30_ì§ì¥_ì¸êµ¬_ìˆ˜',\n",
    "    'ë‚¨ì„±ì—°ë ¹ëŒ€_40_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_50_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ë‚¨ì„±ì—°ë ¹ëŒ€_60_ì´ìƒ_ì§ì¥_ì¸êµ¬_ìˆ˜',\n",
    "    'ì—¬ì„±ì—°ë ¹ëŒ€_10_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_20_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_30_ì§ì¥_ì¸êµ¬_ìˆ˜',\n",
    "    'ì—¬ì„±ì—°ë ¹ëŒ€_40_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_50_ì§ì¥_ì¸êµ¬_ìˆ˜', 'ì—¬ì„±ì—°ë ¹ëŒ€_60_ì´ìƒ_ì§ì¥_ì¸êµ¬_ìˆ˜'\n",
    "]\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ feature ì„ íƒ\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# ë²”ì£¼í˜• featureì˜ ì¸ë±ìŠ¤ ë° ì°¨ì› ì„¤ì •\n",
    "cat_idxs = [feature_cols.index(col) for col in categorical_features if col in feature_cols]\n",
    "cat_dims = [\n",
    "    len(le_district.classes_),\n",
    "    len(le_industry.classes_),\n",
    "    len(le_change.classes_)\n",
    "]\n",
    "\n",
    "print(f\"âœ“ ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\")\n",
    "print(f\"âœ“ ì œì™¸ ì»¬ëŸ¼ ìˆ˜: {len(exclude_cols)}\")\n",
    "print(f\"âœ“ ì‚¬ìš© feature ìˆ˜: {len(feature_cols)}\")\n",
    "print(f\"âœ“ ë²”ì£¼í˜• feature: {len(categorical_features)}ê°œ\")\n",
    "print(f\"  - cat_idxs: {cat_idxs}\")\n",
    "print(f\"  - cat_dims: {cat_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ê¸°ì¤€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "6. ë°ì´í„° ë¶„í•  - ì‹œê³„ì—´ ê¸°ì¤€\n",
      "================================================================================\n",
      "âœ“ ì´ˆê¸° ë¶„ê¸° ì œê±° í›„ Shape: (36875, 202)\n",
      "âœ“ Train: 25812 (70.0%)\n",
      "  - Class 0: 12938, Class 1: 12874\n",
      "âœ“ Validation: 3687 (10.0%)\n",
      "  - Class 0: 1705, Class 1: 1982\n",
      "âœ“ Test: 7376 (20.0%)\n",
      "  - Class 0: 3568, Class 1: 3808\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. ë°ì´í„° ë¶„í•  - ì‹œê³„ì—´ ê¸°ì¤€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. ê²°ì¸¡ê°’ ì œê±°\n",
    "df_clean = df.dropna(subset=feature_cols).reset_index(drop=True)\n",
    "\n",
    "# 2. ê° ê·¸ë£¹ì˜ ì´ˆê¸° ë°ì´í„° ì œê±° (lag2 ìƒì„± ì´í›„ë¶€í„° ì‚¬ìš©)\n",
    "# lag2 ì‚¬ìš©í•  ê²½ìš° ì²˜ìŒ 2ë¶„ê¸°ê¹Œì§€ëŠ” ë°ì´í„°ê°€ ëª¨ë‘ 0ìœ¼ë¡œ ì²˜ë¦¬ë¨\n",
    "df_clean = df_clean.groupby(['ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded'], group_keys=False).apply(\n",
    "    lambda x: x.iloc[2:] if len(x) > 2 else x  # ê° ê·¸ë£¹ì—ì„œ ì²˜ìŒ 2ê°œ ë¶„ê¸° ì œê±°\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 3. ì‹œê³„ì—´ ì •ë ¬\n",
    "df_sorted = df_clean.sort_values(\n",
    "    ['year_quarter', 'ìì¹˜êµ¬_encoded', 'ì—…ì¢…_encoded']\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ“ ì´ˆê¸° ë¶„ê¸° ì œê±° í›„ Shape: {df_sorted.shape}\")\n",
    "\n",
    "# Train/Val/Test ë¶„í• \n",
    "train_size = 0.7\n",
    "val_size = 0.1\n",
    "test_size = 0.2\n",
    "\n",
    "n_total = len(df_sorted)\n",
    "n_train = int(n_total * train_size)\n",
    "n_val = int(n_total * val_size)\n",
    "\n",
    "train_df = df_sorted.iloc[:n_train].copy()\n",
    "val_df = df_sorted.iloc[n_train:n_train+n_val].copy()\n",
    "test_df = df_sorted.iloc[n_train+n_val:].copy()\n",
    "\n",
    "print(f\"âœ“ Train: {len(train_df)} ({len(train_df)/n_total*100:.1f}%)\")\n",
    "print(f\"  - Class 0: {(train_df['closure_risk']==0).sum()}, Class 1: {(train_df['closure_risk']==1).sum()}\")\n",
    "print(f\"âœ“ Validation: {len(val_df)} ({len(val_df)/n_total*100:.1f}%)\")\n",
    "print(f\"  - Class 0: {(val_df['closure_risk']==0).sum()}, Class 1: {(val_df['closure_risk']==1).sum()}\")\n",
    "print(f\"âœ“ Test: {len(test_df)} ({len(test_df)/n_total*100:.1f}%)\")\n",
    "print(f\"  - Class 0: {(test_df['closure_risk']==0).sum()}, Class 1: {(test_df['closure_risk']==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê·¸ë£¹ í†µê³„ëŸ‰ ê³„ì‚° - Train ë°ì´í„°ë§Œ ì‚¬ìš© (Leakage ë°©ì§€)\n",
    "\n",
    "**í•µì‹¬:**\n",
    "- Trainì—ì„œë§Œ ê·¸ë£¹ í‰ê·  ê³„ì‚°\n",
    "- Val/TestëŠ” Trainì˜ í†µê³„ëŸ‰ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "7. ê·¸ë£¹ í†µê³„ëŸ‰ ê³„ì‚° - Train ë°ì´í„°ë§Œ ì‚¬ìš© (Leakage ë°©ì§€)\n",
      "================================================================================\n",
      "âœ“ ìµœì¢… feature ìˆ˜: 66\n",
      "âœ“ ê·¸ë£¹ í†µê³„ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì¶”ê°€: 5ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. ê·¸ë£¹ í†µê³„ëŸ‰ ê³„ì‚° - Train ë°ì´í„°ë§Œ ì‚¬ìš© (Leakage ë°©ì§€)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Trainì—ì„œë§Œ ê·¸ë£¹ í‰ê·  ê³„ì‚°\n",
    "district_sales_mean = train_df.groupby('ìì¹˜êµ¬_encoded')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'].mean()\n",
    "industry_sales_mean = train_df.groupby('ì—…ì¢…_encoded')['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'].mean()\n",
    "\n",
    "# ëª¨ë“  ì„¸íŠ¸ì— ì ìš© (trainì˜ í†µê³„ëŸ‰ ì‚¬ìš©)\n",
    "train_df['ìƒê¶Œ_ë§¤ì¶œ_í‰ê· '] = train_df['ìì¹˜êµ¬_encoded'].map(district_sales_mean)\n",
    "train_df['ì—…ì¢…_ë§¤ì¶œ_í‰ê· '] = train_df['ì—…ì¢…_encoded'].map(industry_sales_mean)\n",
    "\n",
    "val_df['ìƒê¶Œ_ë§¤ì¶œ_í‰ê· '] = val_df['ìì¹˜êµ¬_encoded'].map(district_sales_mean)\n",
    "val_df['ì—…ì¢…_ë§¤ì¶œ_í‰ê· '] = val_df['ì—…ì¢…_encoded'].map(industry_sales_mean)\n",
    "\n",
    "test_df['ìƒê¶Œ_ë§¤ì¶œ_í‰ê· '] = test_df['ìì¹˜êµ¬_encoded'].map(district_sales_mean)\n",
    "test_df['ì—…ì¢…_ë§¤ì¶œ_í‰ê· '] = test_df['ì—…ì¢…_encoded'].map(industry_sales_mean)\n",
    "\n",
    "# ìƒëŒ€ ì§€í‘œ ê³„ì‚°\n",
    "for dataset in [train_df, val_df, test_df]:\n",
    "    dataset['ìƒê¶Œ_ëŒ€ë¹„_ë§¤ì¶œ'] = dataset['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] / (dataset['ìƒê¶Œ_ë§¤ì¶œ_í‰ê· '] + 1)\n",
    "    dataset['ì—…ì¢…_ëŒ€ë¹„_ë§¤ì¶œ'] = dataset['ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡_lag1'] / (dataset['ì—…ì¢…_ë§¤ì¶œ_í‰ê· '] + 1)\n",
    "\n",
    "    # ìœ„í—˜ ì ìˆ˜ (t-1 ì‹œì  ë°ì´í„° ê¸°ë°˜)\n",
    "    dataset['ìœ„í—˜_ì ìˆ˜'] = (\n",
    "        (dataset['ë§¤ì¶œ_ë³€í™”ìœ¨'] < -0.1).astype(int) * 2 +\n",
    "        (dataset['ì„ëŒ€ë£Œ_ë¶€ë‹´ë¥ '] > 0.3).astype(int) * 2 +\n",
    "        (dataset['ìƒê¶Œ_ëŒ€ë¹„_ë§¤ì¶œ'] < 0.7).astype(int) +\n",
    "        (dataset.get('ì—°ì†_ë§¤ì¶œ_ê°ì†Œ', 0) >= 2).astype(int) * 3\n",
    "    )\n",
    "\n",
    "# Feature ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸\n",
    "additional_features = ['ìƒê¶Œ_ë§¤ì¶œ_í‰ê· ', 'ì—…ì¢…_ë§¤ì¶œ_í‰ê· ', 'ìƒê¶Œ_ëŒ€ë¹„_ë§¤ì¶œ', 'ì—…ì¢…_ëŒ€ë¹„_ë§¤ì¶œ', 'ìœ„í—˜_ì ìˆ˜']\n",
    "feature_cols.extend(additional_features)\n",
    "\n",
    "# ë²”ì£¼í˜• ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸\n",
    "cat_idxs = [feature_cols.index(col) for col in categorical_features if col in feature_cols]\n",
    "\n",
    "print(f\"âœ“ ìµœì¢… feature ìˆ˜: {len(feature_cols)}\")\n",
    "print(f\"âœ“ ê·¸ë£¹ í†µê³„ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì¶”ê°€: {len(additional_features)}ê°œ\")\n",
    "\n",
    "# ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "train_df = train_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "val_df = val_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_df = test_df.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### ë°ì´í„° ì €ì¥ - ê³µìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# train_df, val_df, test_dfì—ì„œ feature_colsë§Œ ì¶”ì¶œí•´ì„œ ì €ì¥\n",
    "train_features = train_df[feature_cols]\n",
    "train_target = train_df['closure_risk']\n",
    "\n",
    "val_features = val_df[feature_cols]\n",
    "val_target = val_df['closure_risk']\n",
    "\n",
    "test_features = test_df[feature_cols]\n",
    "test_target = test_df['closure_risk']\n",
    "\n",
    "# ì €ì¥.1 (validation set ë¶„í•  í•„ìš”í•œ ê²½ìš°) - íŒŒì¼ ê²½ë¡œ ìˆ˜ì • í•„ìš”\n",
    "train_features.to_csv('./preprocessed/train_features.csv', index=False)\n",
    "train_target.to_csv('./preprocessed/train_target.csv', index=False)\n",
    "val_features.to_csv('./preprocessed/val_features.csv', index=False)\n",
    "val_target.to_csv('./preprocessed/val_target.csv', index=False)\n",
    "test_features.to_csv('./preprocessed/test_features.csv', index=False)\n",
    "test_target.to_csv('./preprocessed/test_target.csv', index=False)\n",
    "\n",
    "# ì €ì¥.2 (validation set ë”°ë¡œ ë¶„í•  í•„ìš” ì—†ëŠ” ê²½ìš°)\n",
    "# train_val_features = pd.concat([train_features, val_features], axis=0)\n",
    "# train_val_target = pd.concat([train_target, val_target], axis=0)\n",
    "# train_val_features.to_csv('./preprocessed/train_val.csv', index=False)\n",
    "# train_val_target.to_csv('./preprocessed/train_target.csv', index=False)\n",
    "# test_features.to_csv('./preprocessed/test_features.csv', index=False)\n",
    "# test_target.to_csv('./preprocessed/test_target.csv', index=False)\n",
    "\n",
    "print('ë°ì´í„° ì €ì¥ ì™„ë£Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "8. ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§\n",
      "================================================================================\n",
      "âœ“ ì—°ì†í˜• feature ìˆ˜: 63ê°œ\n",
      "âœ“ ë²”ì£¼í˜• feature ìˆ˜: 3ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. ë°ì´í„° ì¤€ë¹„ ë° ìŠ¤ì¼€ì¼ë§\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë°ì´í„° ì¶”ì¶œ\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['closure_risk'].values\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['closure_risk'].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['closure_risk'].values\n",
    "\n",
    "# ë²”ì£¼í˜•ê³¼ ì—°ì†í˜• ë¶„ë¦¬\n",
    "categorical_mask = np.zeros(len(feature_cols), dtype=bool)\n",
    "for idx in cat_idxs:\n",
    "    categorical_mask[idx] = True\n",
    "continuous_mask = ~categorical_mask\n",
    "\n",
    "# Feature ìŠ¤ì¼€ì¼ë§ (ì—°ì†í˜• ë³€ìˆ˜ë§Œ)\n",
    "scaler = StandardScaler()\n",
    "X_train_continuous = X_train[:, continuous_mask].astype(np.float64)\n",
    "X_val_continuous = X_val[:, continuous_mask].astype(np.float64)\n",
    "X_test_continuous = X_test[:, continuous_mask].astype(np.float64)\n",
    "\n",
    "X_train_continuous_scaled = scaler.fit_transform(X_train_continuous)\n",
    "X_val_continuous_scaled = scaler.transform(X_val_continuous)\n",
    "X_test_continuous_scaled = scaler.transform(X_test_continuous)\n",
    "\n",
    "# ë²”ì£¼í˜•ê³¼ ìŠ¤ì¼€ì¼ë§ëœ ì—°ì†í˜• ê²°í•©\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[:, continuous_mask] = X_train_continuous_scaled\n",
    "X_val_scaled[:, continuous_mask] = X_val_continuous_scaled\n",
    "X_test_scaled[:, continuous_mask] = X_test_continuous_scaled\n",
    "\n",
    "# MPS í˜¸í™˜ì„± \n",
    "if device == 'mps':\n",
    "    X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "    X_val_scaled = X_val_scaled.astype(np.float32)\n",
    "    X_test_scaled = X_test_scaled.astype(np.float32)\n",
    "\n",
    "print(f\"âœ“ ì—°ì†í˜• feature ìˆ˜: {continuous_mask.sum()}ê°œ\")\n",
    "print(f\"âœ“ ë²”ì£¼í˜• feature ìˆ˜: {categorical_mask.sum()}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TabNet ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "9. TabNet ëª¨ë¸ í•™ìŠµ\n",
      "================================================================================\n",
      "\n",
      "âœ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
      "\n",
      "epoch 0  | loss: 0.75571 | val_0_accuracy: 0.61242 | val_0_auc: 0.65446 | val_0_logloss: 0.67075 |  0:00:09s\n",
      "epoch 1  | loss: 0.67328 | val_0_accuracy: 0.65989 | val_0_auc: 0.71916 | val_0_logloss: 0.643   |  0:00:13s\n",
      "epoch 2  | loss: 0.61778 | val_0_accuracy: 0.67426 | val_0_auc: 0.75827 | val_0_logloss: 0.60785 |  0:00:17s\n",
      "epoch 3  | loss: 0.589   | val_0_accuracy: 0.69243 | val_0_auc: 0.77229 | val_0_logloss: 0.57571 |  0:00:22s\n",
      "epoch 4  | loss: 0.58014 | val_0_accuracy: 0.69731 | val_0_auc: 0.7779  | val_0_logloss: 0.57062 |  0:00:26s\n",
      "epoch 5  | loss: 0.58336 | val_0_accuracy: 0.69026 | val_0_auc: 0.77348 | val_0_logloss: 0.56576 |  0:00:30s\n",
      "epoch 6  | loss: 0.57852 | val_0_accuracy: 0.69406 | val_0_auc: 0.77818 | val_0_logloss: 0.56393 |  0:00:34s\n",
      "epoch 7  | loss: 0.57034 | val_0_accuracy: 0.69053 | val_0_auc: 0.78459 | val_0_logloss: 0.56442 |  0:00:39s\n",
      "epoch 8  | loss: 0.56697 | val_0_accuracy: 0.69325 | val_0_auc: 0.78464 | val_0_logloss: 0.55819 |  0:00:43s\n",
      "epoch 9  | loss: 0.56602 | val_0_accuracy: 0.69081 | val_0_auc: 0.78766 | val_0_logloss: 0.56005 |  0:00:47s\n",
      "epoch 10 | loss: 0.56707 | val_0_accuracy: 0.6927  | val_0_auc: 0.78289 | val_0_logloss: 0.55705 |  0:00:51s\n",
      "epoch 11 | loss: 0.56666 | val_0_accuracy: 0.70057 | val_0_auc: 0.78734 | val_0_logloss: 0.55207 |  0:00:56s\n",
      "epoch 12 | loss: 0.56352 | val_0_accuracy: 0.69406 | val_0_auc: 0.78801 | val_0_logloss: 0.55422 |  0:01:00s\n",
      "epoch 13 | loss: 0.55932 | val_0_accuracy: 0.69243 | val_0_auc: 0.78868 | val_0_logloss: 0.55653 |  0:01:04s\n",
      "epoch 14 | loss: 0.55704 | val_0_accuracy: 0.70464 | val_0_auc: 0.7924  | val_0_logloss: 0.54832 |  0:01:08s\n",
      "epoch 15 | loss: 0.55767 | val_0_accuracy: 0.69731 | val_0_auc: 0.79175 | val_0_logloss: 0.55095 |  0:01:12s\n",
      "epoch 16 | loss: 0.55969 | val_0_accuracy: 0.69596 | val_0_auc: 0.78841 | val_0_logloss: 0.5527  |  0:01:17s\n",
      "epoch 17 | loss: 0.55855 | val_0_accuracy: 0.69596 | val_0_auc: 0.79036 | val_0_logloss: 0.55448 |  0:01:21s\n",
      "epoch 18 | loss: 0.55214 | val_0_accuracy: 0.70328 | val_0_auc: 0.79207 | val_0_logloss: 0.5486  |  0:01:25s\n",
      "epoch 19 | loss: 0.55243 | val_0_accuracy: 0.70681 | val_0_auc: 0.79267 | val_0_logloss: 0.54589 |  0:01:30s\n",
      "epoch 20 | loss: 0.5524  | val_0_accuracy: 0.70274 | val_0_auc: 0.79336 | val_0_logloss: 0.54782 |  0:01:34s\n",
      "epoch 21 | loss: 0.5528  | val_0_accuracy: 0.69433 | val_0_auc: 0.79222 | val_0_logloss: 0.55546 |  0:01:38s\n",
      "epoch 22 | loss: 0.55008 | val_0_accuracy: 0.70545 | val_0_auc: 0.79458 | val_0_logloss: 0.5468  |  0:01:42s\n",
      "epoch 23 | loss: 0.5479  | val_0_accuracy: 0.69976 | val_0_auc: 0.79653 | val_0_logloss: 0.55028 |  0:01:46s\n",
      "epoch 24 | loss: 0.54514 | val_0_accuracy: 0.71169 | val_0_auc: 0.79871 | val_0_logloss: 0.54105 |  0:01:50s\n",
      "epoch 25 | loss: 0.54423 | val_0_accuracy: 0.69894 | val_0_auc: 0.79953 | val_0_logloss: 0.54583 |  0:01:55s\n",
      "epoch 26 | loss: 0.54152 | val_0_accuracy: 0.71359 | val_0_auc: 0.80061 | val_0_logloss: 0.54202 |  0:01:59s\n",
      "epoch 27 | loss: 0.54015 | val_0_accuracy: 0.7144  | val_0_auc: 0.79993 | val_0_logloss: 0.54022 |  0:02:03s\n",
      "epoch 28 | loss: 0.53873 | val_0_accuracy: 0.7163  | val_0_auc: 0.80197 | val_0_logloss: 0.537   |  0:02:07s\n",
      "epoch 29 | loss: 0.53686 | val_0_accuracy: 0.71657 | val_0_auc: 0.80108 | val_0_logloss: 0.53706 |  0:02:12s\n",
      "epoch 30 | loss: 0.53629 | val_0_accuracy: 0.71006 | val_0_auc: 0.80256 | val_0_logloss: 0.54055 |  0:02:16s\n",
      "epoch 31 | loss: 0.53286 | val_0_accuracy: 0.71711 | val_0_auc: 0.80243 | val_0_logloss: 0.53963 |  0:02:20s\n",
      "epoch 32 | loss: 0.53307 | val_0_accuracy: 0.71549 | val_0_auc: 0.80164 | val_0_logloss: 0.54372 |  0:02:24s\n",
      "epoch 33 | loss: 0.53073 | val_0_accuracy: 0.71928 | val_0_auc: 0.80162 | val_0_logloss: 0.53729 |  0:02:29s\n",
      "epoch 34 | loss: 0.53045 | val_0_accuracy: 0.72281 | val_0_auc: 0.80372 | val_0_logloss: 0.53731 |  0:02:33s\n",
      "epoch 35 | loss: 0.53024 | val_0_accuracy: 0.72172 | val_0_auc: 0.80359 | val_0_logloss: 0.53529 |  0:02:37s\n",
      "epoch 36 | loss: 0.53052 | val_0_accuracy: 0.71901 | val_0_auc: 0.80432 | val_0_logloss: 0.53959 |  0:02:41s\n",
      "epoch 37 | loss: 0.52759 | val_0_accuracy: 0.72335 | val_0_auc: 0.80394 | val_0_logloss: 0.53679 |  0:02:46s\n",
      "epoch 38 | loss: 0.528   | val_0_accuracy: 0.72118 | val_0_auc: 0.80427 | val_0_logloss: 0.53441 |  0:02:50s\n",
      "epoch 39 | loss: 0.52717 | val_0_accuracy: 0.72118 | val_0_auc: 0.80514 | val_0_logloss: 0.53473 |  0:02:54s\n",
      "epoch 40 | loss: 0.5277  | val_0_accuracy: 0.72417 | val_0_auc: 0.80915 | val_0_logloss: 0.53183 |  0:02:59s\n",
      "epoch 41 | loss: 0.52575 | val_0_accuracy: 0.72634 | val_0_auc: 0.8072  | val_0_logloss: 0.53036 |  0:03:04s\n",
      "epoch 42 | loss: 0.52456 | val_0_accuracy: 0.73474 | val_0_auc: 0.81061 | val_0_logloss: 0.52518 |  0:03:08s\n",
      "epoch 43 | loss: 0.52424 | val_0_accuracy: 0.72498 | val_0_auc: 0.8072  | val_0_logloss: 0.53017 |  0:03:12s\n",
      "epoch 44 | loss: 0.52308 | val_0_accuracy: 0.72742 | val_0_auc: 0.80794 | val_0_logloss: 0.53451 |  0:03:16s\n",
      "epoch 45 | loss: 0.52322 | val_0_accuracy: 0.72851 | val_0_auc: 0.81043 | val_0_logloss: 0.5282  |  0:03:21s\n",
      "epoch 46 | loss: 0.52145 | val_0_accuracy: 0.72688 | val_0_auc: 0.80882 | val_0_logloss: 0.53505 |  0:03:25s\n",
      "epoch 47 | loss: 0.51922 | val_0_accuracy: 0.73176 | val_0_auc: 0.81188 | val_0_logloss: 0.52482 |  0:03:29s\n",
      "epoch 48 | loss: 0.51865 | val_0_accuracy: 0.72688 | val_0_auc: 0.80875 | val_0_logloss: 0.52981 |  0:03:33s\n",
      "epoch 49 | loss: 0.51825 | val_0_accuracy: 0.71956 | val_0_auc: 0.80909 | val_0_logloss: 0.53389 |  0:03:38s\n",
      "epoch 50 | loss: 0.51773 | val_0_accuracy: 0.72796 | val_0_auc: 0.81046 | val_0_logloss: 0.52824 |  0:03:42s\n",
      "epoch 51 | loss: 0.51767 | val_0_accuracy: 0.72335 | val_0_auc: 0.81092 | val_0_logloss: 0.52921 |  0:03:46s\n",
      "epoch 52 | loss: 0.51681 | val_0_accuracy: 0.72634 | val_0_auc: 0.80894 | val_0_logloss: 0.52985 |  0:03:51s\n",
      "epoch 53 | loss: 0.51679 | val_0_accuracy: 0.72715 | val_0_auc: 0.8096  | val_0_logloss: 0.52816 |  0:03:55s\n",
      "epoch 54 | loss: 0.51625 | val_0_accuracy: 0.71983 | val_0_auc: 0.80815 | val_0_logloss: 0.53281 |  0:04:00s\n",
      "epoch 55 | loss: 0.51498 | val_0_accuracy: 0.72335 | val_0_auc: 0.8094  | val_0_logloss: 0.52993 |  0:04:04s\n",
      "epoch 56 | loss: 0.51587 | val_0_accuracy: 0.72823 | val_0_auc: 0.80969 | val_0_logloss: 0.52569 |  0:04:08s\n",
      "epoch 57 | loss: 0.51584 | val_0_accuracy: 0.72145 | val_0_auc: 0.80782 | val_0_logloss: 0.531   |  0:04:14s\n",
      "epoch 58 | loss: 0.51482 | val_0_accuracy: 0.71711 | val_0_auc: 0.80766 | val_0_logloss: 0.53495 |  0:04:18s\n",
      "epoch 59 | loss: 0.51423 | val_0_accuracy: 0.72091 | val_0_auc: 0.80743 | val_0_logloss: 0.53284 |  0:04:23s\n",
      "epoch 60 | loss: 0.51422 | val_0_accuracy: 0.72579 | val_0_auc: 0.80769 | val_0_logloss: 0.53246 |  0:04:28s\n",
      "epoch 61 | loss: 0.51537 | val_0_accuracy: 0.72389 | val_0_auc: 0.80866 | val_0_logloss: 0.53203 |  0:04:32s\n",
      "epoch 62 | loss: 0.51297 | val_0_accuracy: 0.72118 | val_0_auc: 0.8077  | val_0_logloss: 0.53398 |  0:04:36s\n",
      "epoch 63 | loss: 0.51212 | val_0_accuracy: 0.72254 | val_0_auc: 0.80985 | val_0_logloss: 0.53163 |  0:04:41s\n",
      "epoch 64 | loss: 0.5106  | val_0_accuracy: 0.72471 | val_0_auc: 0.80795 | val_0_logloss: 0.5325  |  0:04:45s\n",
      "epoch 65 | loss: 0.51084 | val_0_accuracy: 0.72552 | val_0_auc: 0.81142 | val_0_logloss: 0.53104 |  0:04:49s\n",
      "epoch 66 | loss: 0.50973 | val_0_accuracy: 0.722   | val_0_auc: 0.80738 | val_0_logloss: 0.52968 |  0:04:53s\n",
      "epoch 67 | loss: 0.5108  | val_0_accuracy: 0.72715 | val_0_auc: 0.80661 | val_0_logloss: 0.535   |  0:04:58s\n",
      "epoch 68 | loss: 0.50983 | val_0_accuracy: 0.72688 | val_0_auc: 0.80917 | val_0_logloss: 0.53046 |  0:05:02s\n",
      "epoch 69 | loss: 0.50855 | val_0_accuracy: 0.72769 | val_0_auc: 0.80877 | val_0_logloss: 0.52622 |  0:05:06s\n",
      "epoch 70 | loss: 0.50818 | val_0_accuracy: 0.72389 | val_0_auc: 0.80936 | val_0_logloss: 0.52928 |  0:05:10s\n",
      "epoch 71 | loss: 0.50948 | val_0_accuracy: 0.722   | val_0_auc: 0.81054 | val_0_logloss: 0.53024 |  0:05:15s\n",
      "epoch 72 | loss: 0.50768 | val_0_accuracy: 0.72715 | val_0_auc: 0.81145 | val_0_logloss: 0.52724 |  0:05:19s\n",
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 47 and best_val_0_logloss = 0.52482\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7dcac0f398be>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nâœ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_importance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# compute feature importance once the best model is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_compute_feature_importances\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \"\"\"\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mM_explain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0msum_explain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_explain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mfeature_importances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_explain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_explain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, X, normalize)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mM_explain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. TabNet ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = TabNetClassifier(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=3,\n",
    "    n_d=8,\n",
    "    n_a=8,\n",
    "    n_steps=3,\n",
    "    gamma=1.4,\n",
    "    lambda_sparse=0.000009,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.01),\n",
    "    scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    seed=SEED,\n",
    "    verbose=1,\n",
    "    device_name=device,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘\\n\")\n",
    "model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    eval_metric=['accuracy', 'auc', 'logloss'],\n",
    "    max_epochs=150,\n",
    "    patience=25,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=256\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ í•™ìŠµ ì™„ë£Œ\")\n",
    "print(f\"  - Best Epoch: {model.best_epoch}\")\n",
    "print(f\"  - Best Loss: {model.best_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ í‰ê°€ - Data Leakage ì œê±° í›„ ì‹¤ì œ ì„±ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. ëª¨ë¸ í‰ê°€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
    "y_val_proba = model.predict_proba(X_val_scaled)\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_proba[:, 1])\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_threshold = roc_thresholds[optimal_idx]\n",
    "\n",
    "print(f\"âœ“ ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "y_test_proba = model.predict_proba(X_test_scaled)\n",
    "y_test_pred = (y_test_proba[:, 1] >= optimal_threshold).astype(int)\n",
    "\n",
    "# í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "auc = roc_auc_score(y_test, y_test_proba[:, 1])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ìµœì¢… ì„±ëŠ¥':^80}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  AUC:       {auc:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"\\nâœ“ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nâœ“ Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Low Risk', 'High Risk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Low Risk', 'High Risk'],\n",
    "    yticklabels=['Low Risk', 'High Risk'],\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Confusion Matrix', fontsize=14, pad=20)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"11. Feature Importance (Top 20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Top 20) ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.barh(range(len(importance_df)), importance_df['importance'])\n",
    "ax.set_yticks(range(len(importance_df)))\n",
    "ax.set_yticklabels(importance_df['feature'], fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Top 20 Feature Importance', fontsize=14, pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
